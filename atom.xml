<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title> - Articles</title>
    <description>Machine Learning, NLP</description>
    <link>
    </link>
    
      
      <item>
        <title>ACL 2020 Highlights</title>
        
          <description>&lt;p&gt;This post discusses highlights of the main conference of the 2020 Annual Meeting of the Association for Computational Linguistics (ACL 2020). The conference accepted 779 papers with an acceptance rate of 22.7%, had 25 tracks along with demo sessions, virtual meetups and mentoring sessions.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 17 Jul 2020 21:03:36 +0000</pubDate>
        <link>
        /acl2020</link>
        <guid isPermaLink="true">/acl2020</guid>
      </item>
      
    
      
      <item>
        <title>ACL 2019 Highlights</title>
        
          <description>&lt;p&gt;This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 01 Aug 2019 21:03:36 +0000</pubDate>
        <link>
        /acl2019</link>
        <guid isPermaLink="true">/acl2019</guid>
      </item>
      
    
      
      <item>
        <title>mcQA - Multiple Choice Question Answering</title>
        
          <description>&lt;p&gt;mcQA is a multiple choice question answering python library, using Language Models.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 10 Jul 2019 07:25:36 +0000</pubDate>
        <link>
        /mcQA</link>
        <guid isPermaLink="true">/mcQA</guid>
      </item>
      
    
      
      <item>
        <title>Building Parallel Corpora Using Cross-Lingual BOW</title>
        
          <description>&lt;p&gt;Training machine translation models requires a huge amount of parallel data.
Consequently, there has been many works suggesting different methods to build
bilingual corpora, leading to the construction of reliable training datasets for
machine translation systems.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 13 Jul 2018 07:25:36 +0000</pubDate>
        <link>
        /Building-Parallel-Corpora-CLBOW</link>
        <guid isPermaLink="true">/Building-Parallel-Corpora-CLBOW</guid>
      </item>
      
    
      
      <item>
        <title>Clause Augmentation for Better NMT</title>
        
          <description>&lt;p&gt;Most public parallel corpora are formed of long sentences. Consequently, neural translation models tend to generate a long output with n-grams repetition, even when they are exposed to a short sequence or a one-word example. This causes the repetition problem, explained by the fact that none of the neurons learns the representation of length, thus the model generates a long sequence by default. In other terms, the probability of appearance of the end-of-sentence token &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eos&amp;gt;&lt;/code&gt; will not be high enough to stop the output generation when translating a short sequences.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 01 Apr 2018 07:25:36 +0000</pubDate>
        <link>
        /Clause-Augmentation-for-Better-NMT</link>
        <guid isPermaLink="true">/Clause-Augmentation-for-Better-NMT</guid>
      </item>
      
    
  </channel>
</rss>
