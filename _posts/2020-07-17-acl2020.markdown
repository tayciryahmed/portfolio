---
layout: post
current: post
cover: assets/images/seattle.jpg
navigation: True
title: ACL 2020 Highlights: Evaluation, Interpretability and more.
date: 2020-07-17 21:03:36
tags:
class: post-template
subclass: 'post'
author: tayciryahmed
---


This post discusses highlights of the main conference of the 2020 Annual Meeting of the Association for Computational Linguistics (ACL 2020). The conference accepted 779 papers with an acceptance rate of 22.7%, had 25 tracks along with demo sessions, virtual meetups and mentoring sessions. 

For the first time ever, this year's ACL conference has a special theme with a dedicated session and award: **Taking Stock of Where We've Been and Where We're Going**. The theme surely highlights the current state of the field. Indeed, the NLP research community has been pushing boundries in terms of performance ([GLUE](https://gluebenchmark.com/leaderboard/) benchmark), innovative architectures (transformer, BERT & co), quick access to SOTA (HuggingFace's [transformers](https://github.com/huggingface/transformers)) and model sizes ([GPT-3:](https://arxiv.org/abs/2005.14165) 175B parameters). As declared by [Clement Delangue](https://twitter.com/ClementDelangue/status/1283411618395815936), Co-Founder and CEO of HuggingFace: "NLP is going to be the most transformational tech of the decade!" But at this growth pace, ACL Program Co-Chairs deemed healthy for the research community to take a step back and reflect on the current state of the field, to avoid getting stuck in suboptimal solutions and consciously chart out the roadmap for future research directions. 

In addition to the insightful theme choice, the Co-Chairs introduced 4 additional tracks including: [**Ethics and NLP** & **Interpretability and Analysis of Models for NLP**](https://acl2020.org/blog/the-first-call-for-papers-is-out/). Indeed, after many sessions and workshops in the previous \*CL events, the community is making it clear that responsible and ethical models are crucial as more NLP models are deployed in production and driving decisions that impact people's lives. Moreover, as research works continue to push performance boudries, interpreting and analysing models have become more relevant in order to understand the models inner mechanisms and learn about the secret ingredients for their performance. 

We can see in the graph below that the most popular tracks in ACL 2020 are (1) Interpretability and Analysis of Models for NLP, (2) Theme, (3) Cognitive Modeling and Psycholinguistics and (4) Ethics and NLP.

![Views box-plot by area, sorted by median (by Yoav Goldberg)]({{site.baseurl}}assets/images/acl2020graph.png)

*Note that the graph corresponds to a [snapshot](https://twitter.com/yoavgo/status/1282459579339681792) of the views on Jul 13, 2020. For readability, 3 outliers were removed with 416 (Linzen et al), 533 (Gururangan et al) and 970 (Bender and Koller) views.*

Below are the topics on which I focused this year: 

* Theme and Robust Evaluation
* Analysis and Interpretability

## Theme and Robust Evaluation 

The theme *Taking Stock of Where We've Been and Where We're Going* illustrates the desire of the NLP community to make sure the current trend of training Machine Learning models to solve NLP problems, does not lead to a locally optimal state of the field's progress. This takes us back to the 90s, when introducing statistical methods to computational linguistics raised discussions between protagonists from both sides claiming the advantages of each approach. But, this is still a very interesting approach, since taking this step back to reflect on what the community needs to tackle, should put it back on track to building more insightful and fair models. As a matter of fact, during the past year, we have seen a trend of training ever bigger transformers on ever bigger datasets to break benchmark leaderboards, in a metrics-obsessed approach to research. This trend comes with its costs, financial ([GPT-3](https://arxiv.org/abs/2005.14165) costs $12 million), but also environmental and ethical. Indeed, so few groups have access to as much computational power and resources, which narrows the research competition. It also raises the question if bigger-is-better a scientific approach. To [quote](https://twitter.com/fchollet/status/1122330598968705025) François Chollet: "Training ever bigger convnets and LSTMs on ever bigger datasets gets us closer to Strong AI -- in the same sense that building taller towers gets us closer to the moon". Though, it is agreed upon that these methods are the result of impressive engineering acheivements, e.g. [Turing-NLG](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/) is the result of developing [`DeepSpeed`](https://github.com/microsoft/DeepSpeed), a deep learning optimization library for distributed training. Moreover, they are promissing methodologies for zero, one and few shots scopes, that are useful for many NLP applications. Nonetheless, the community is clearly having an existential moment and doing introspection to bypass this metrics-driven paradigm, in the quest to achieve human NLP ability in machines. Indeed, several talks argue that focusing on metrics drives us away from real linguistic challenges in the distribution's tail and that models, though beat human performance with respect to metrics, are unable to handle trivial example for the human mind. For instance, [Bender and Koller](https://www.aclweb.org/anthology/2020.acl-main.463.pdf) show in their *theme wining* paper that meaning cannot be learnt from form. As a consquence, "the language modeling task, because it only uses form as training data, cannot in principle lead to learning of meaning". They also shared a word of caution in their talk about using terms like "comprehension", "understanding" and "meaning" when describing models capabilities. 

Over the last few years, the main paradigm of many research papers has been to train large scale models and evaluate them on test sets that are similar to the training sets, leading many to believe that we are solving datasets not tasks. Indeed, in her keynote Kathy McKeown emphasizes that leaderboards are not always helpful to advance the field, because benchmarks capture the head of the distribution whreas the most challenging aspect are in its tail. The keynote goes through the present, past and future of the field as percieved by some of the most renowned NLP researchers. In particular, McKeown tries to depict the current state of the field being dominated by the achievements of deep neural networks. As a matter of fact, DNN enable us to robustly solve many applications compared to other models and ensure a good performance using simpler models for tasks like machine translation, summurization and dialogue, making it simpler to deploy, build and democratize AI. Other researchers also praise the ingenuity of the attention mechanism and the success of generative models compared to n-gram approches, in addition to the undeniable success of DNN in NLP benchmarks and representation learning. But, if we take a step back to the past of NLP to visualize the bigger picture, we find that the field used to rely extensively on looking at individual examples and little details. The evaluation used to be limited to manully examining outputs at a small scale and seek relevant conclusions in the distribution's tail. Thus, McKeown puts the building blocks for the future by inviting the research community to focus on tasks cannot be solved by deep learning or those for which we need to develop new methods. She also called for bringing data back to NLP by looking closely at examples, carefully analyzing them, and solving challenging problems that matter not just for which we have pre-built data sets. As for language generation, she points out that neural generators do not speak with purpose, say what they mean, choose words, form sentence structures intentionally or plan long text, like humans do. Finally, she concluded with the importance of the interdisciplinary parts of language, and advises that we should put more effort into the interpretability and analysis of outputs.



* best paper
* other papers on evaluation  


## Analysis and Interpretability

* right to explanation https://en.wikipedia.org/wiki/Right_to_explanation
* Tutorial + papers : done 

* I believe that interpretability is essential to language research, and it’s important to understand models and their outputs to make applications more robust and reliable.


## See also
* [Selection of papers from ACL-2020](https://docs.google.com/document/d/1rQYAjY-jNKoQh8Z9-4NjqLF1_nAD7amr4sYH62eGPbU/edit#heading=h.lrgn79ao0for) by Yacine Jernite (HuggingFace)
* [Interpretability and Analysis of Models for NLP @ ACL 2020](https://medium.com/@lawrence.carolin/interpretability-and-analysis-of-models-for-nlp-e6b977ac1dc6) by Carolin Lawrence
* [Highlights of ACL 2020](https://medium.com/analytics-vidhya/highlights-of-acl-2020-4ef9f27a4f0c) by Vered Shwartz (AI2 & University of Washington)
* [Ten emerging topics at ACL 2020](https://europe.naverlabs.com/blog/ten-emerging-topics-at-acl-2020/) by NAVER LABS
* [Knowledge Graphs in Natural Language Processing @ ACL 2020](https://towardsdatascience.com/knowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1) by Michael Galkin


*Acknowledgment: Thanks to Mohamed Ali Jamaoui for being a first reader and giving helpful feedback.* 
