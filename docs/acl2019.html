<!DOCTYPE html>
<html>
<head>

    <!-- Document Settings -->
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />

    <!-- Base Meta -->
    <!-- dynamically fixing the title for tag/author pages -->



    <title>ACL 2019 Highlights</title>
    <meta name="HandheldFriendly" content="True" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <!-- Styles'n'Scripts -->
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/screen.edited.css" />
    <link rel="stylesheet" type="text/css" href="/assets/built/syntax.css" />
    <!-- highlight.js -->
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <style>.hljs { background: none; }</style>

    <!--[if IE]>
        <style>
            p, ol, ul{
                width: 100%;
            }
            blockquote{
                width: 100%;
            }
        </style>
    <![endif]-->
    
    <!-- This tag outputs SEO meta+structured data and other important settings -->
    <meta name="description" content="Machine Learning, NLP" />
    <link rel="shortcut icon" href="/assets/images/favicon.png" type="image/png" />
    <link rel="canonical" href="/acl2019" />
    <meta name="referrer" content="no-referrer-when-downgrade" />

     <!--title below is coming from _includes/dynamic_title-->
    <meta property="og:site_name" content="Taycir Yahmed" />
    <meta property="og:type" content="website" />
    <meta property="og:title" content="ACL 2019 Highlights" />
    <meta property="og:description" content="This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind. The conference accepted 660 papers with an acceptance rate of 22.7% and had over 3K participants. It was" />
    <meta property="og:url" content="/acl2019" />
    <meta property="og:image" content="/assets/images/acl.jpg" />
    <meta property="article:publisher" content="https://www.facebook.com/" />
    <meta property="article:author" content="https://www.facebook.com/" />
    <meta property="article:published_time" content="2019-08-01T16:03:36-05:00" />
    <meta property="article:modified_time" content="2019-08-01T16:03:36-05:00" />
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="ACL 2019 Highlights" />
    <meta name="twitter:description" content="This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind. The conference accepted 660 papers with an acceptance rate of 22.7% and had over 3K participants. It was" />
    <meta name="twitter:url" content="/" />
    <meta name="twitter:image" content="/assets/images/acl.jpg" />
    <meta name="twitter:label1" content="Written by" />
    <meta name="twitter:data1" content="Taycir Yahmed" />
    <meta name="twitter:site" content="@TaycirYahmed" />
    <meta name="twitter:creator" content="@TaycirYahmed" />
    <meta property="og:image:width" content="1400" />
    <meta property="og:image:height" content="933" />

    <script type="application/ld+json">
{
    "@context": "https://schema.org",
    "@type": "Website",
    "publisher": {
        "@type": "Organization",
        "name": "Taycir Yahmed",
        "logo": "/"
    },
    "url": "/acl2019",
    "image": {
        "@type": "ImageObject",
        "url": "/assets/images/acl.jpg",
        "width": 2000,
        "height": 666
    },
    "mainEntityOfPage": {
        "@type": "WebPage",
        "@id": "/acl2019"
    },
    "description": "This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind. The conference accepted 660 papers with an acceptance rate of 22.7% and had over 3K participants. It was"
}
    </script>

    <!-- <script type="text/javascript" src="https://demo.ghost.io/public/ghost-sdk.min.js?v=724281a32e"></script>
    <script type="text/javascript">
    ghost.init({
    	clientId: "ghost-frontend",
    	clientSecret: "f84a07a72b17"
    });
    </script> -->

    <meta name="generator" content="Jekyll 3.6.2" />
    <link rel="alternate" type="application/rss+xml" title="ACL 2019 Highlights" href="/feed.xml" />


</head>
<body class="post-template">

    <div class="site-wrapper">
        <!-- All the main content gets inserted here, index.hbs, post.hbs, etc -->
        <!-- default -->

<!-- The tag above means: insert everything in this file
into the {body} of the default.hbs template -->

<header class="site-header outer">
    <div class="inner">
        <nav class="site-nav">
    <div class="site-nav-left">
        
            
                <a class="site-nav-logo" href="/">Taycir Yahmed</a>
            
        
        
            <ul class="nav" role="menu">
    <li class="nav-home" role="menuitem"><a href="/">Home</a></li>
    <li class="nav-about" role="menuitem"><a href="/about/">About</a></li>
    <!--<li class="nav-getting-started" role="menuitem"><a href="/tag/getting-started/">Getting Started</a></li>
    <li class="nav-try-ghost" role="menuitem"><a href="https://ghost.org">Try Ghost</a></li>-->
</ul>

        
    </div>
    <div class="site-nav-right">
        <div class="social-links">
            
            
                <a class="social-link social-link-tw" href="https://twitter.com/TaycirYahmed" target="_blank" rel="noopener"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>
</a>
            
        </div>
        
    </div>
</nav>

    </div>
</header>

<!-- Everything inside the #post tags pulls data from the post -->
<!-- #post -->

<main id="site-main" class="site-main outer" role="main">
    <div class="inner">

        <article class="post-full post ">

            <header class="post-full-header">
                <section class="post-full-meta">
                    <time class="post-full-meta-date" datetime=" 1 August 2019"> 1 August 2019</time>
                    
                </section>
                <h1 class="post-full-title">ACL 2019 Highlights</h1>
            </header>

            
            <figure class="post-full-image" style="background-image: url(/assets/images/acl.jpg)">
            </figure>
            

            <section class="post-full-content">
                <div class="kg-card-markdown">
                    <p>This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind.</p>

<p>The conference accepted 660 <a href="http://www.acl2019.org/EN/program.xhtml">papers</a> with an acceptance rate of 22.7% and had over 3K participants. It was sponsored by various industrial big players from the NLP research community (<a href="http://www.acl2019.org/EN/exhibitors-sponsors.xhtml">Full list of sponsors</a>).</p>

<p>The presidential address tackled several topics including the following list, that stuck out with me most:</p>

<ul>
  <li>Error analysis, interpretability and reproducibility</li>
  <li>Multilinguality, low resource tasks and domain adaptation</li>
  <li>Commonsense, reasoning and context aware modeling</li>
  <li>Ethical NLP: Bias and environmental impact</li>
</ul>

<h2 id="error-analysis-interpretability-and-reproducibility">Error analysis, interpretability and reproducibility</h2>
<p>Deep Learning models are very sensitive to noisy user input. In translation for instance, these variations of the user input can reveal bias in the language modeling and training data. To limit these drawbacks, <a href="https://arxiv.org/pdf/1906.02443.pdf">Cheng et al.</a> (Google) suggests perturbing the model with adversarial inputs using an algorithm named <strong>Adversarial Generation</strong> (AdvGen), which generates plausible adversarial examples and feeds them back into the model. This idea is inspired by the logic behind GANs but it doesn’t rely on a discriminator: instead it simply <em>augments</em> the training data with these adversarial examples (more on AdvGen <a href="https://ai.googleblog.com/2019/07/robust-neural-machine-translation.html">here</a>).</p>

<p>Interpretability also keeps coming back in different papers, showing a general interest in understanding and explaining the models’ inside mechanisms. <a href="https://arxiv.org/abs/1906.03731">Serrano et al.</a> argues that attention wights don’t always identify information that models find important. Many experiments were performed proving that attention weights mostly don’t correlate with model outputs; whereas gradient-based rankings of attention weights better predict their effects (more in future papers). On the other hand, <a href="https://www.aclweb.org/anthology/P19-1284">Bastings et al.</a> show that sparse re-parameterized samples and unbiased gradients lead to effective latent rationales <em>(a short and informative part of the input text)</em> for text classification. The approach consists in training two models, one latent model that selects the rationale to input in the second one that performs the classification task.</p>

<p>Equally, many papers try to explain models capabilities and low level representations. For instance, <a href="https://arxiv.org/abs/1906.00592">Yang et al.</a> show that SANs (self-attention networks, popular for their parallelization and strong performance on various NLP tasks, such as machine translation) can’t efficiently learn the positional information even with the position embedding, when trained on a word reordering detection task. Nevertheless, SANs learn better positional information than RNNs when trained on a different downstream task such as MT. When it comes to <em>Multi-head selef-attention</em>, <a href="https://arxiv.org/abs/1905.09418">Voita et al.</a> show that the most important (in magnitude) and confident heads play consistent and often linguistically interpretable roles. Furthermore, they prove that pruning heads (using a method based on stochastic gates and a differentiable relaxation of the L0 penalty) leadsto remove the vast majority without (or with minimum) impact on performance. More generally, <a href="https://hal.inria.fr/hal-02131630/document">Jawaharet et al.</a> (Inria) investigate which structure of language BERT learns. They show that in the lower layers BERT learns phrase-level information, in the intermediate layers a hierarchy of linguistic information (surface features at the bottom, syntactic features in the middle and semantic features at the top). They also prove that BERT requires deeper layers to encode long-distance information (e.g. track subject-verb agreement).</p>

<p>Further details and resources about interpretability will be shared in the <a href="https://blackboxnlp.github.io/">BlackboxNLP</a> workshop, following the main conference. Live streaming <a href="https://www.youtube.com/embed/JEH2fiyjrJU">link</a>.</p>

<p>On another note, there is a growing interest in <strong>robust evaluation</strong> and reproducibility among the NLP community. Indeed, the benchmark leaderboards race often comes with small core changes and “public hyper-parameters search with more training data” (e.g. RoBERTa), which leads to over-fitting benchmark datasets and drives the attention away from more fundamental research and new idea. One of the selected “outstanding papers” deals with issue: <a href="http://wellformedness.com/papers/gorman-bedrick-2019.pdf">Gorman et al.</a> show that system rankings based on standard splits fail to reproduce, and recommend “Bonferroni-corrected random split hypothesis testing” instead. The best demo paper also provides an open-source framework for machine translation quality estimation called <a href="https://unbabel.github.io/OpenKiwi">OpenKiwi</a>.</p>

<h2 id="multilinguality-low-resource-tasks-and-domain-adaptation">Multilinguality, low resource tasks and domain adaptation</h2>
<p>Since last year, an interest in multilingual training to support low-resource languages has be pronounced. Indeed, most of the NLP benchmarks and SOTA results are reported on English, whereas in real life applications other languages are used and needed. That led a wave of cross-lingual and multilingual research results. <a href="https://arxiv.org/abs/1906.05407">Ormazabal et al.</a> argue that most cross-lingual embeddings are learned with off-line methods (learn embeddings in different languages then map them to a shared space) which underlay the <em>isomorphism assumption</em>, stating that embeddings in different languages have the same structure. They investigate whether this issue is due to the mapping or learning of embeddings. Eventually, they prove that joint learning (using an extension of skip-gram model with bilingual data) yields more isomorphic embeddings and better bilingual lexicon induction. Thus, they conclude that mapping methods have strong limitation and call for more research in joint learning. Indeed, so many research group have been interested in aligning cross-lingual embeddings to build bilingual dictionaries and induce word translation pairs through nearest neighbor or related retrieval methods. Furthermore, <a href="https://arxiv.org/abs/1907.10761">Artetxe et al.</a> suggest inducing bilingual embeddings through a different approach: It consist of generating a synthetic parallel corpus  through a phrase based translation system (built using cross-lingual embeddings) and then extract the bilingual lexicon using statistical word alignment methods. Thus, they use no more resources than the monolingual corpus used to learn the embeddings.</p>

<p>The works on cross-lingual and multi-lingual NLP led the community to adopt a more stimulating position about presenting works in various languages. The Bender Rule (ref. Emily M. Bender) states that any work should state the language they use and not fall to English by default, since that emphasizes English as a reference language (though it doesn’t capture multilingual variations) and puts other languages in a second stand. <a href="https://arxiv.org/abs/1906.04726">Mielke, et al.</a> investigate languages that are harder to model, giving the observation that SOTA methods do not perform equally well on high resource languages (from the Europarl corpus). To do that, they start by defining an objective measure of difficulty using parallel corpora covering 69 languages (13 language families) and propose a paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients, handling missing data and inter-sentence variations. On the other hand, <a href="https://arxiv.org/abs/1902.00193">Rahimi et al.</a> propose a “massively” multilingual transfer for NER models, applied to emergency response. They show that, when using cross-lingual embeddings, some language pairs ensure better performance than the ensemble voting models, yielding unexpected language pairs (Indonesian is best transfered from Italian, indicating that these results depend on the used embeddings). Although, the transfer results are best within language families, they remain overall noisy, thus they suggest applying transfer from multiple source languages. The main idea is to encode the input sentence to a cross-lingual representation, then use models trained on high resource languages with cross-lingual representations, to predict the tags. <a href="https://docs.google.com/presentation/d/13y31FNC-LTwENdOiWUajVeusuE-sFgcmz3hO8qHdh90/">link</a> to the slides. In another work about multilingual BERT, <a href="https://arxiv.org/abs/1906.01502">Pires et al.</a> show that transfer doesn’t really depend on vocabulary overlap but rather on typological similarity (SVO vs. SOV), indeed they noticed a performance drop when changing word order. Further more, they prove that mBERT is good at transfer mixed-language (code switching) but not transliterated targets. Finally, they confirm that translations have similar representations.</p>

<p>Furthermore, in the presidential address, Ming Zhou listed the topics to work on low-resource tasks including: <strong>Transfer learning, Unsupervised learning, Cross-language learning and Prior knowledge with human in the loop.</strong> For instance, <a href="https://arxiv.org/abs/1905.08212">Wang et al.</a> suggest using the most related high resource language to improve low resource NMT, instead of using all available multilingual data. Moreover, they prove that using an <em>intelligent</em> selection method from other auxiliary languages can further enhance the performance. To do so, they propose an algorithm dabbed Target Conditioned Sampling (TCS), which first samples a target sentence, and then conditionally samples its source sentence. On a different note, <a href="https://www.cis.uni-muenchen.de/~fraser/pubs/huck_acl2019.pdf">Huck et al.</a> propose an approach to better translate OOVs (Out Of Vocabulary words) in MT systems. Indeed, these words are usually represented with BPE tokens, but that can lead to bad translations, for languages like German. The main idea, here, is to translate the OOVs using bilingual embeddings and keeping track of 5 candidate translations, then back-translate the targets while forcing the target OOV to translate to the source OOV. By fine-tuning the model with this synthetic data, they report better OOVs translation performance. Moreover, <a href="https://arxiv.org/pdf/1906.03785.pdf">Xia et al.</a> suggest an approach to use monolingual data when training low-resource translation models that pivots through a <strong>related</strong> high-resource language (HRL). First, they inject LRL words in HRL sentences using bilingual dictionaries. Second, they edit the modified sentences using an unsupervised MT framework.  They show that the proposed method outperforms back-translation. As of speech translation, recent papers showed that it was possible to achieve with end-to-end models (avoiding cascaded models: STT -&gt; MT) but considering the <em>unrealistic</em> assumption of equal size of training data. <a href="https://arxiv.org/abs/1904.07209">Sperber et al.</a> show that end-to-end speech translation models require more data to achieve the same performance as cascaded models. To solve this problem, they propose training end-to-end multi-task models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. Furthermore, they introduce an attention-passing technique that alleviates error propagation issues.  Moreover, in an attempt to reuse elaborated methods in low-resource tasks,  <a href="https://arxiv.org/abs/1905.09135">Beryozkin et al.</a> (Google) use a tag hierarchy to adapt a pre-trained NER model with no additional training data. The main idea consists in constructing a tag hierarchy, training the model with the highest level of tags as targets and back-propagating with the fine-grained tags.</p>

<h2 id="commonsense-reasoning-and-context-aware-modeling">Commonsense, reasoning and context aware modeling</h2>
<p>This topic comes back in each session dealing with QA; it seems to be more challenging and with a certain latency between the research results and the industry applications. Indeed, the research community is focusing more on seq2seq models - that do not ensure a perfect user experience both in chit-chat and task oriented systems, consequently industrial systems rely more on NLU, DST, DM and NLG components. In the second keynote, Pascal Fung explained various aspects related to QA systems and the ethical challenges they raise, including presenting them as humans deceiving the user at the other end, in certain use cases.</p>

<p>Research groups in companies as well as in academia are focusing efforts on building meaningful datasets, intended to make QA / reasoning systems closer to the end-user, for example Google Research has presented a dataset paper with user formulated requests matched to their answers from wikipedia articles with two fine grained levels : paragraph containing the answer and entity corresponding to the exact answer to the question. This is pushing end-to-end systems to deal directly with the user’s query and infer the exact information to look for. <a href="https://ai.google.com/research/NaturalQuestions/">Link</a> to the data. Furthermore, <a href="https://arxiv.org/pdf/1905.07098.pdf">Xiong et al.</a> propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (KB) and a set of retrieved text snippets. On the other hand, <a href="https://arxiv.org/pdf/1906.04980.pdf">Lewis et al.</a> follow the insights from Lample et el. on unsupervised machine translation, to investigate unsupervised extractive QA. They suggest various methods to perform cloze to natural question translation and show that modern QA systems have a good performance even when trained using only synthetic data. Moreover, <a href="https://arxiv.org/abs/1905.13453">Talmor et al.</a>  investigate generalization and transfer in reading comprehension tasks and show that training on a source RC dataset and transferring to a target dataset substantially improves performance. They propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on 5 RC datasets.</p>

<p>There seems to be an important interest in building systems with in-domain data that ensure context aware predictions. Studies show that systems lacking context in real life applications (Away from the SOTA and leaderboard benchmarks on very specific and studied datasets) can lead to biased decision making and erroneous predictions. <a href="https://homes.cs.washington.edu/~skgabrie/sap2019risk.pdf">Sap et al.</a> show that systems trained on out of domain data lead to a strongly biased decisions and dangerous censorship.</p>

<p>On a different note, the transformer-xl paper by <a href="https://arxiv.org/abs/1901.02860">Dai et al.</a> was presented during the conference. The main idea consists of surpassing the fixed-length context, (a long text sequence is truncated into fixed-length segments processed separately), using two techniques: a <em>segment-level recurrence mechanism</em> and a <em>relative positional encoding scheme</em>. More details <a href="https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html">here</a>. Furthermore, the first keynote by Liang Huang introduce a new architecture dabbed prefix-to-prefix as an evolution of seq-to-seq to take into consideration the temporal evolution of the captured context in simultaneous translation (real time). More in the <a href="https://arxiv.org/abs/1906.01135">paper</a>.</p>

<h2 id="ethical-nlp-bias-and-environmental-impact">Ethical NLP: Bias and environmental impact</h2>
<p>There was a significant number of papers dealing with debiasing NLP methods at ACL 2019. Indeed, recent studies have shown that pre-trained word embeddings and NLP resources hold a certain level of gender and geographical bias, among others. Since these models can be used in decision making models, ethical concerns have emerged, illustrated by the wide interest of the research community. For instance, <a href="https://arxiv.org/abs/1906.08976">Sun et al.</a> present a literature overview of gender bias mitigating techniques. Furthermore, <a href="https://arxiv.org/abs/1906.04571">Zmigrod et al.</a> show that commonly employed approaches produce ungrammatical sentences in morphologically rich languages and present a novel approach for converting between masculine-inflected and feminine-inflected sentences. They test their approach on 4 languages showing bias reduction without harming grammaticality. On demographic bias, <a href="https://www.aclweb.org/anthology/P19-1162">Sweeney et al.</a> argue that most demographic bias evaluation approaches rely on vector space based metrics like the <em>Word Embedding Association Test (WEAT)</em>, which doesn’t measure the impact on downstream tasks. They suggest a new metric (Relative Negative Sentiment Bias, RNSB) to measure the relative negative sentiment associated with demographic identity terms. Moreover, a position paper by <a href="https://arxiv.org/abs/1906.01738">Jurgens et al.</a> argues that the community needs to make three substantive changes to address online abuse: first, expanding the scope of problems to tackle both more subtle and more serious forms of abuse, second, developing proactive technologies that counter or inhibit abuse before it harms, and third reframing our effort within a framework of justice to promote healthy communities.</p>

<p>More details on the workshop for <a href="https://genderbiasnlp.talp.cat/">Gender Bias in Natural Language Processing</a>.</p>

<p>An other interesting paper by [Strubell et al.] illustrates the environmental impact of training and especially tuning SOTA NLP models. The authors quantify the approximate financial and environmental costs of training a variety of recent neural network models for NLP. Then, they propose actionable recommendations to reduce costs and improve equity in NLP research and practice.</p>

<h2 id="see-also">See also</h2>
<ul>
  <li><a href="https://www.livecongress.it/sved/evt/aol_lnk.php?id=60B5FD70">Video recordings</a></li>
  <li><a href="http://www.cs.cmu.edu/~neulab//2019/07/25/neulab-presentations-at-acl-2019.html">NeuLab Presentations at ACL 2019</a></li>
  <li><a href="https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.p">Unsupervised Cross-lingual Representation Learning tutorial</a></li>
  <li>Best papers nominations <a href="http://www.acl2019.org/EN/nominations-for-acl-2019-best-paper-awards.xhtml">here</a>.</li>
  <li><a href="https://www.mihaileric.com/posts/nlp-trends-acl-2019/">Trends in Natural Language Processing: ACL 2019 In Review</a> by Mihail Eric.</li>
  <li><a href="https://medium.com/@mgalkin/knowledge-graphs-in-natural-language-processing-acl-2019-7a14eb20fce8">Knowledge Graphs in Natural Language Processing @ ACL 2019</a> by Michael Galkin.</li>
  <li><a href="http://noecasas.com/post/acl2019/">Notes on ACL 2019</a> by Noe Casas.</li>
</ul>

                </div>
            </section>

            <!-- Email subscribe form at the bottom of the page -->
            

            <footer class="post-full-footer">
                <!-- Everything inside the #author tags pulls data from the author -->
                <!-- #author-->
                
                    
                        <section class="author-card">
                            
                                <img class="author-profile-image" src="/assets/images/ghost.png" alt="taycir" />
                            
                            <section class="author-card-content">
                                <h4 class="author-card-name"><a href="/author/taycir">Taycir Yahmed</a></h4>
                                
                                    <p>Machine Learning, NLP</p>
                                
                            </section>
                        </section>
                        <div class="post-full-footer-right">
                            <a class="author-card-button" href="/author/taycir">Read More</a>
                        </div>
                    
                
                <!-- /author  -->
            </footer>

            <!-- If you use Disqus comments, just uncomment this block.
            The only thing you need to change is "test-apkdzgmqhj" - which
            should be replaced with your own Disqus site-id. -->
            

        </article>

    </div>
</main>

<!-- Links to Previous/Next posts -->
<aside class="read-next outer">
    <div class="inner">
        <div class="read-next-feed">
            

            <!-- If there's a next post, display it using the same markup included from - partials/post-card.hbs -->
            

            <!-- If there's a previous post, display it using the same markup included from - partials/post-card.hbs -->
            
                

    <article class="post-card post-template">
        
            <a class="post-card-image-link" href="/mcQA">
                <div class="post-card-image" style="background-image: url(/assets/images/writing.jpg)"></div>
            </a>
        
        <div class="post-card-content">
            <a class="post-card-content-link" href="/mcQA">
                <header class="post-card-header">
                    

                    <h2 class="post-card-title">mcQA - Multiple Choice Question Answering</h2>
                </header>
                <section class="post-card-excerpt">
                    
                        <p></p>
                    
                </section>
            </a>
            <footer class="post-card-meta">
                
                    
                        
                        <img class="author-profile-image" src="/assets/images/ghost.png" alt="Taycir Yahmed" />
                        
                        <span class="post-card-author">
                            <a href="/author/taycir/">Taycir Yahmed</a>
                        </span>
                    
                
                <span class="reading-time">
                    
                    
                      1 min read
                    
                </span>
            </footer>
        </div>
    </article>

            

        </div>
    </div>
</aside>

<!-- Floating header which appears on-scroll, included from includes/floating-header.hbs -->
<div class="floating-header">
    <div class="floating-header-logo">
        <a href="/">
            
                <img src="/assets/images/favicon.png" alt="Taycir Yahmed icon" />
            
            <span>Taycir Yahmed</span>
        </a>
    </div>
    <span class="floating-header-divider">&mdash;</span>
    <div class="floating-header-title">ACL 2019 Highlights</div>
    <div class="floating-header-share">
        <div class="floating-header-share-label">Share this <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24">
    <path d="M7.5 15.5V4a1.5 1.5 0 1 1 3 0v4.5h2a1 1 0 0 1 1 1h2a1 1 0 0 1 1 1H18a1.5 1.5 0 0 1 1.5 1.5v3.099c0 .929-.13 1.854-.385 2.748L17.5 23.5h-9c-1.5-2-5.417-8.673-5.417-8.673a1.2 1.2 0 0 1 1.76-1.605L7.5 15.5zm6-6v2m-3-3.5v3.5m6-1v2"/>
</svg>
</div>
        <a class="floating-header-share-tw" href="https://twitter.com/share?text=ACL+2019+Highlights&amp;url=https://tayciryahmed.comacl2019"
            onclick="window.open(this.href, 'share-twitter', 'width=550,height=235');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M30.063 7.313c-.813 1.125-1.75 2.125-2.875 2.938v.75c0 1.563-.188 3.125-.688 4.625a15.088 15.088 0 0 1-2.063 4.438c-.875 1.438-2 2.688-3.25 3.813a15.015 15.015 0 0 1-4.625 2.563c-1.813.688-3.75 1-5.75 1-3.25 0-6.188-.875-8.875-2.625.438.063.875.125 1.375.125 2.688 0 5.063-.875 7.188-2.5-1.25 0-2.375-.375-3.375-1.125s-1.688-1.688-2.063-2.875c.438.063.813.125 1.125.125.5 0 1-.063 1.5-.25-1.313-.25-2.438-.938-3.313-1.938a5.673 5.673 0 0 1-1.313-3.688v-.063c.813.438 1.688.688 2.625.688a5.228 5.228 0 0 1-1.875-2c-.5-.875-.688-1.813-.688-2.75 0-1.063.25-2.063.75-2.938 1.438 1.75 3.188 3.188 5.25 4.25s4.313 1.688 6.688 1.813a5.579 5.579 0 0 1 1.5-5.438c1.125-1.125 2.5-1.688 4.125-1.688s3.063.625 4.188 1.813a11.48 11.48 0 0 0 3.688-1.375c-.438 1.375-1.313 2.438-2.563 3.188 1.125-.125 2.188-.438 3.313-.875z"/></svg>

        </a>
        <a class="floating-header-share-fb" href="https://www.facebook.com/sharer/sharer.php?u=https://tayciryahmed.comacl2019"
            onclick="window.open(this.href, 'share-facebook','width=580,height=296');return false;">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 32 32"><path d="M19 6h5V0h-5c-3.86 0-7 3.14-7 7v3H8v6h4v16h6V16h5l1-6h-6V7c0-.542.458-1 1-1z"/></svg>

        </a>
    </div>
    <progress class="progress" value="0">
        <div class="progress-container">
            <span class="progress-bar"></span>
        </div>
    </progress>
</div>


<!-- /post -->

<!-- The #contentFor helper here will send everything inside it up to the matching #block helper found in default.hbs -->


        <!-- Previous/next page links - displayed on every page -->
        

        <!-- The footer at the very bottom of the screen -->
        <footer class="site-footer outer">
            <div class="site-footer-content inner">
                <section class="copyright"><a href="/">Taycir Yahmed</a> &copy; 2019</section>
                <nav class="site-footer-nav">
                    <a href="/">Latest Posts</a>
                    <a href="https://github.com/tayciryahmed" target="_blank" rel="noopener">GitHub</a>
                    <a href="https://twitter.com/TaycirYahmed" target="_blank" rel="noopener">Twitter</a>
                </nav>
            </div>
        </footer>

    </div>

    <!-- The big email subscribe modal content -->
    

    <!-- highlight.js -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.10.0/components/prism-abap.min.js"></script>
    <script>$(document).ready(function() {
      $('pre code').each(function(i, block) {
        hljs.highlightBlock(block);
      });
    });</script>

    <!-- jQuery + Fitvids, which makes all video embeds responsive -->
    <script
        src="https://code.jquery.com/jquery-3.2.1.min.js"
        integrity="sha256-hwg4gsxgFZhOsEEamdOYGBf13FyQuiTwlAQgxVSNgt4="
        crossorigin="anonymous">
    </script>
    <script type="text/javascript" src="/assets/js/jquery.fitvids.js"></script>
    <script type="text/javascript" src="https://demo.ghost.io/assets/js/jquery.fitvids.js?v=724281a32e"></script>


    <!-- Paginator increased to "infinit" in _config.yml -->
    <!-- if paginator.posts  -->
    <!-- <script>
        var maxPages = parseInt('');
    </script>
    <script src="/assets/js/infinitescroll.js"></script> -->
    <!-- /endif -->

    


    <!-- Add Google Analytics  -->
    <!-- Google Analytics Tracking code -->
 <script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-145236815-1', 'auto');
  ga('send', 'pageview');

 </script>


    <!-- The #block helper will pull in data from the #contentFor other template files. In this case, there's some JavaScript which we only want to use in post.hbs, but it needs to be included down here, after jQuery has already loaded. -->
    
        <script>

// NOTE: Scroll performance is poor in Safari
// - this appears to be due to the events firing much more slowly in Safari.
//   Dropping the scroll event and using only a raf loop results in smoother
//   scrolling but continuous processing even when not scrolling
$(document).ready(function () {
    // Start fitVids
    var $postContent = $(".post-full-content");
    $postContent.fitVids();
    // End fitVids

    var progressBar = document.querySelector('progress');
    var header = document.querySelector('.floating-header');
    var title = document.querySelector('.post-full-title');

    var lastScrollY = window.scrollY;
    var lastWindowHeight = window.innerHeight;
    var lastDocumentHeight = $(document).height();
    var ticking = false;

    function onScroll() {
        lastScrollY = window.scrollY;
        requestTick();
    }

    function onResize() {
        lastWindowHeight = window.innerHeight;
        lastDocumentHeight = $(document).height();
        requestTick();
    }

    function requestTick() {
        if (!ticking) {
            requestAnimationFrame(update);
        }
        ticking = true;
    }

    function update() {
        var trigger = title.getBoundingClientRect().top + window.scrollY;
        var triggerOffset = title.offsetHeight + 35;
        var progressMax = lastDocumentHeight - lastWindowHeight;

        // show/hide floating header
        if (lastScrollY >= trigger + triggerOffset) {
            header.classList.add('floating-active');
        } else {
            header.classList.remove('floating-active');
        }

        progressBar.setAttribute('max', progressMax);
        progressBar.setAttribute('value', lastScrollY);

        ticking = false;
    }

    window.addEventListener('scroll', onScroll, {passive: true});
    window.addEventListener('resize', onResize, false);

    update();
});
</script>

    

    <!-- Ghost outputs important scripts and data with this tag - it should always be the very last thing before the closing body tag -->
    <!-- ghost_foot -->

</body>
</html>
