<?xml version="1.0" encoding="utf-8"?>

<feed xmlns="http://www.w3.org/2005/Atom" >
  <generator uri="https://jekyllrb.com/" version="3.7.4">Jekyll</generator>
  <link href="/author/tayciryahmed/feed.xml" rel="self" type="application/atom+xml" />
  <link href="/" rel="alternate" type="text/html" />
  <updated>2020-07-21T17:22:04+00:00</updated>
  <id>/author/tayciryahmed/feed.xml</id>

  
  
  

  
    <title type="html">Taycir Yahmed | </title>
  

  
    <subtitle>Machine Learning, NLP</subtitle>
  

  

  
    
      
    
  

  
  

  
    <entry>
      <title type="html">ACL 2020 Highlights</title>
      <link href="/acl2020" rel="alternate" type="text/html" title="ACL 2020 Highlights" />
      <published>2020-07-17T21:03:36+00:00</published>
      <updated>2020-07-17T21:03:36+00:00</updated>
      <id>/acl2020</id>
      <content type="html" xml:base="/acl2020">&lt;p&gt;This post discusses highlights of the main conference of the 2020 Annual Meeting of the Association for Computational Linguistics (ACL 2020). The conference accepted 779 papers with an acceptance rate of 22.7%, had 25 tracks along with demo sessions, virtual meetups and mentoring sessions.&lt;/p&gt;

&lt;p&gt;For the first time ever, this year’s ACL conference has a special theme with a dedicated session and award: &lt;strong&gt;Taking Stock of Where We’ve Been and Where We’re Going&lt;/strong&gt;. The theme surely highlights the current state of the field. Indeed, the NLP research community has been pushing boundaries in terms of performance (&lt;a href=&quot;https://gluebenchmark.com/leaderboard/&quot;&gt;GLUE&lt;/a&gt; benchmark), innovative architectures (transformer, BERT &amp;amp; co), quick access to SOTA (HuggingFace’s &lt;a href=&quot;https://github.com/huggingface/transformers&quot;&gt;transformers&lt;/a&gt;) and model sizes (&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;GPT-3:&lt;/a&gt; 175B parameters). As declared by &lt;a href=&quot;https://twitter.com/ClementDelangue/status/1283411618395815936&quot;&gt;Clement Delangue&lt;/a&gt;, Co-Founder and CEO of HuggingFace: “NLP is going to be the most transformational tech of the decade!” But at this growth pace, ACL Program Co-Chairs deemed healthy for the research community to take a step back and reflect on the current state of the field, to avoid getting stuck in suboptimal solutions and consciously chart out the roadmap for future research directions.&lt;/p&gt;

&lt;p&gt;In addition to the insightful theme choice, the Co-Chairs introduced 4 additional tracks including: &lt;a href=&quot;https://acl2020.org/blog/the-first-call-for-papers-is-out/&quot;&gt;&lt;strong&gt;Ethics and NLP&lt;/strong&gt; &amp;amp; &lt;strong&gt;Interpretability and Analysis of Models for NLP&lt;/strong&gt;&lt;/a&gt;. Indeed, after many sessions and workshops in the previous *CL events, the community is making it clear that responsible and ethical models are crucial as more NLP models are deployed in production and driving decisions that impact people’s lives. Moreover, as research works continue to push performance boundaries, interpreting and analysing models have become more relevant in order to understand the models inner mechanisms and learn about the secret ingredients for their performance.&lt;/p&gt;

&lt;p&gt;We can see in the box-plot of views by area, sorted by median (credit: Yoav Goldberg) below, that the most popular tracks in ACL 2020 are (1) Interpretability and Analysis of Models for NLP, (2) Theme, (3) Cognitive Modeling and Psycholinguistics and (4) Ethics and NLP.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/acl2020graph.png&quot; alt=&quot;Views box-plot by area, sorted by median (by Yoav Goldberg)&quot; width=&quot;600&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Note that the graph corresponds to a &lt;a href=&quot;https://twitter.com/yoavgo/status/1282459579339681792&quot;&gt;snapshot&lt;/a&gt; of the views on Jul 13, 2020. For readability, 3 outliers were removed with 416 (Linzen et al), 533 (Gururangan et al) and 970 (Bender and Koller) views.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Below are the topics on which I focused this year:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Theme and Robust Evaluation&lt;/li&gt;
  &lt;li&gt;Analysis and Interpretability&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;theme-and-robust-evaluation&quot;&gt;Theme and Robust Evaluation&lt;/h2&gt;

&lt;p&gt;The theme &lt;em&gt;Taking Stock of Where We’ve Been and Where We’re Going&lt;/em&gt; illustrates the desire of the NLP community to make sure the current trend of training Machine Learning models to solve NLP problems, does not lead to a locally optimal state of the field’s progress. This takes us back to the 90s, when introducing statistical methods to computational linguistics raised discussions between protagonists from both sides claiming the advantages of each approach. But, this is still a very interesting approach, since taking this step back to reflect on what the community needs to tackle, should put it back on track to building more insightful and fair models. As a matter of fact, during the past year, we have seen a trend of training ever bigger transformers on ever bigger datasets to break benchmark leaderboards, in a metrics-obsessed approach to research. This trend comes with its costs, financial (&lt;a href=&quot;https://arxiv.org/abs/2005.14165&quot;&gt;GPT-3&lt;/a&gt; costs $12 million), but also environmental and ethical. Indeed, so few groups have access to as much computational power and resources, which narrows the research competition. It also raises the question if bigger-is-better a scientific approach. To &lt;a href=&quot;https://twitter.com/fchollet/status/1122330598968705025&quot;&gt;quote&lt;/a&gt; François Chollet: “Training ever bigger convnets and LSTMs on ever bigger datasets gets us closer to Strong AI – in the same sense that building taller towers gets us closer to the moon”. Though, it is agreed upon that these methods are the result of impressive engineering achievements, e.g. &lt;a href=&quot;https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/&quot;&gt;Turing-NLG&lt;/a&gt; is the result of developing &lt;a href=&quot;https://github.com/microsoft/DeepSpeed&quot;&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;DeepSpeed&lt;/code&gt;&lt;/a&gt;, a deep learning optimization library for distributed training. Moreover, they are promising methodologies for zero, one and few shots learning, which is useful for many NLP applications. Nonetheless, the community is clearly having an existential moment and doing some introspection to bypass this metrics-driven paradigm, in the quest to achieve human NLP ability in machines. Indeed, several talks argue that focusing on metrics drives us away from real linguistic challenges in the distribution’s tail and that models, though beat human performance with respect to metrics, are unable to handle trivial examples for the human mind. For instance, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.463.pdf&quot;&gt;Bender and Koller&lt;/a&gt; show in their &lt;em&gt;theme winning&lt;/em&gt; paper that meaning cannot be learnt from form. As a consequence, “the language modeling task, because it only uses form as training data, cannot in principle lead to learning of meaning”. They also shared a word of caution in their talk about using terms like “comprehension”, “understanding” and “meaning” when describing models capabilities.&lt;/p&gt;

&lt;p&gt;Over the last few years, the main paradigm of many research papers has been to train large scale models and evaluate them on test sets that are similar to the training sets, leading many to believe that we are solving datasets not tasks. Indeed, in her keynote Kathy McKeown emphasizes that leaderboards are not always helpful to advance the field, because benchmarks capture the head of the distribution whereas the most challenging aspects are in its tail. The keynote goes through the present, past and future of the field as perceived by some of the most renowned NLP researchers. In particular, McKeown tries to depict the current state of the field being dominated by the achievements of deep neural networks. As a matter of fact, DNNs enable us to robustly solve many applications compared to other models and ensure a good performance using simpler models for tasks like machine translation, summarization and dialogue, making it simpler to deploy, build and democratize AI. Other researchers also praise the ingenuity of the attention mechanism and the success of generative models compared to n-gram approaches, in addition to the undeniable success of DNN in NLP benchmarks and representation learning. But, if we take a step back to the past of NLP to visualize the bigger picture, we find that the field used to rely extensively on looking at individual examples and little details. The evaluation used to be limited to manually examining outputs at a small scale and seek relevant conclusions in the distribution’s tail. Thus, McKeown puts the building blocks for the future by inviting the research community to focus on tasks that cannot be solved by deep learning or those for which we need to develop new methods. She also called for bringing data back to NLP by looking closely at examples, carefully analyzing them, and solving challenging problems that matter not just for which we have pre-built data sets. As for language generation, she points out that neural generators do not speak with purpose, say what they mean, choose words, form sentence structures intentionally or plan long text, like humans do. Finally, she concluded with the importance of the interdisciplinary parts of language, and advised that we should put more effort into the interpretability and analysis of outputs.&lt;/p&gt;

&lt;p&gt;Along with the theme, evaluation is an important track in ACL 2020 with numerous thought-provoking papers. Many tackled the need to look at the data and analyze the errors when evaluating models. In her Lifetime Achievement Award speech, Bonnie Webber suggests that even something as trivial as looking at both precision and recall instead of F1 score can help in understanding the model’s weaknesses and strengths. In machine translation, for instance, it has been a common practice to compare machine outputs to human translations and calculate the BLEU score (dating back to 2002). However, in recent years, many works suggest the inadequacy of such a metric and call for more robust evaluation of machine translation systems. &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.448/&quot;&gt;Mathur et al.&lt;/a&gt; highlight potential problems in the current best practices for assessing evaluation metrics and show that current methods are highly sensitive to the used translations. In addition, the overall best paper by &lt;a href=&quot;https://homes.cs.washington.edu/~marcotcr/acl20_checklist.pdf&quot;&gt;Ribeiro et al.&lt;/a&gt; shows that while evaluating generalization via held-out test sets is useful, it has drawbacks including the overestimation of a model’s capability and the inability to determine its pain points. Thus, they propose a more comprehensive evaluation methodology which is model and task-agnostic, inspired from unit testing in software engineering and dubbed CheckList. The approach illustrates the behavioral testing principle of “decoupling testing from implementation” by treating the model as a black box, allowing the comparison of different models trained on different data sets. The authors also &lt;a href=&quot;https://github.com/marcotcr/checklist&quot;&gt;provide&lt;/a&gt; templates and other abstractions, allowing the users to generate a large number of test cases easily.&lt;/p&gt;

&lt;h2 id=&quot;analysis-and-interpretability&quot;&gt;Analysis and Interpretability&lt;/h2&gt;

&lt;p&gt;Interpretability is absolutely essential to natural language processing, in order to understand models but also make applications more robust, fair and reliable. Indeed, the end-to-end fashion is predominant in language applications, requiring more efforts on accountability, trust, bias, and &lt;a href=&quot;https://en.wikipedia.org/wiki/Right_to_explanation&quot;&gt;right to explanation&lt;/a&gt;. The Interpretability and Analysis in Neural NLP &lt;a href=&quot;https://slideslive.com/38928626/interpretability-and-analysis-in-neural-nlp&quot;&gt;tutorial&lt;/a&gt; by Belinkov, Gehrmann and Pavlick gives an overview of the toolbox of interpretability. First, structural analysis suggests ways to understand what the model has learnt in its internal representations. The main approach is to use probing classifiers, which prove that a layer from a network trained to do a task A can predict a feature B. For example, A can be machine translation and B some linguistic features such as POS or morphological tags. Another idea is to change the activations so that the verb’s tense is changed, and afterwards measure the probability of predicting the pronouns he/she to check for gender bias. Although straightforward, these classifiers can be hard to evaluate, because there is no clear performance level for benchmarking, one can use SOTA or simply baselines. There is also recent work evaluating probing classifiers using code length, i.e. the number of bits to encode the probe. Another limitation of structural analysis can be the lack of causality or correlation between the probe and original model. As for behavioral analysis, they highlight tail phenomena through fine-grained checks and reward models that handle these hard exemples. This approach actually has a long history since the 90s, with ideas like designing test sets that are more adapted to ensure the model does not simply learn some artifacts. To do so, we can select test examples that are less likely to appear in train or simply replace words in sentences. The goal of this method is to make sure the models’ behavior is consistent with the examples they are given, and determine if the model fails in systematic ways. Besides, it is agnostic to the model’s structure and type since it focuses on creating challenge sets or probing sets, showcasing subject verb agreement, negation, antonyms, and other linguistic aspects. To design the challenge dataset, three methods are suggested. First, we can consider tight control by selecting pairs that are the same except for the phenomena we want to study, e.g. gender, negation or conjugation. Second, there is loose control by selecting many examples that illustrate a phenomena, then averaging on this set of interest. Third, we can choose adversarial examples with the objective of tricking the model. In addition, we can either construct these sets manually by having the models predict samples, then construct hard ones to trick it; or in a semi automatic fashion by filtering from an existing corpora, or in a complete automatic way. Nonetheless, these methods present limitations since they don’t determine if the models failure is due to its structure or the training data. Last but not least, there is visualization analysis, which allow us to understand the models, debug them, generate and test hypothesis about their behavior. However, these methods are harder to open source since they usually depend on the use case.&lt;/p&gt;

&lt;p&gt;For the first time ever, ACL co-chairs introduced the “Interpretability and Analysis” track this year, yet it accepted an impressive number of 36 papers. Among these papers, a common observation is that looking at attention weights has gone out of fashion. In contrast, many works demonstrate that attention weights do not provide reliable explanations. For instance, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.432&quot;&gt;Pruthi et al.&lt;/a&gt; cast doubt on attention’s reliability as a tool for auditing algorithms and proved that it can deceive humans into thinking that a model is unbiased with regards to gender even though it is biased. Besides, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.312&quot;&gt;Sun et al.&lt;/a&gt; make the observation that some words receive higher attention weights whilst other relevant words don’t receive high weights and if attention weights are not so interpretable, this does not lead to bad performance. Other papers suggest approaches such as providing the training examples that influenced the prediction the most as an explanation. &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.492&quot;&gt;Han et al.&lt;/a&gt; mention that we usually use gradients, saliency maps, Lime or attention for explanations. They suggest using influence functions which are basically the product of saliency maps with the gradient, and show that these functions are consistent for NLI and sentiment analysis and help identifying artifacts in the training set. In addition, some papers include human users in the interpretability process. On the one hand, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.491&quot;&gt;Hase and Bansal&lt;/a&gt; measure the effect of an explainability method as its effect on Simulatability: “A model is simulatable when users can predict its outputs”. They evaluate several explanation methods, such as feature importance, case-based and latent space traversal; and conclude that Lime is a good approach for tabular data. On the other hand, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.494&quot;&gt;Chen et al.&lt;/a&gt; suggest a hierarchical explanation of a model’s predictions by splitting the sequence at the lowest point given a score measuring the words interactions. The method proves to be better than shapley and lime in terms of coherence between human predictions given explanations and the model’s predictions. They argue that the previous methods are token based, thus we need a higher level explanation. Moreover, faithfulness is an important aspect required in interpretability methods. &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.386&quot;&gt;Jacovi and Goldberg&lt;/a&gt; list the multiple attributes we would like in an interpretation: readability (easy to understand), plausibility (can convince us of the result) and faithfulness (accurately describe the true reasoning of the model). They note that there is clearly a trade-off between faithfulness and readability: NN activations are faithful but not readable. Furthermore, the authors offer three guidelines for faithful interpretations: (1) Faithfulness is not Plausibility, (2) Evaluating interpretation using human input is plausibility not faithfulness, and (3) Claims are just claims until tested. They also state a list of assumptions that prove the interpretations are not faithful. For example, if models predict the same output, they have the same reasoning process, if a model makes a similar decision, its reasoning is similar, and that certain parts of the input can be significant to the decision independently from other parts. Furthermore, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.382&quot;&gt;Camburu et al.&lt;/a&gt; note that generated natural language explanations are not necessarily faithful, i.e. they are inconsistent with the actual reasoning process of the model. They introduce a sanity checking framework for models robustness against generating inconsistent explanations. The main idea consists in training a reverse model ExplainAndPredictAttention that, given an explanation, generates the hypothesis then gets the explanation and checks if it is consistent with the original explanation. Last but not least, &lt;a href=&quot;https://www.aclweb.org/anthology/2020.acl-main.408&quot;&gt;DeYoung et al.&lt;/a&gt; introduce a benchmark to evaluate interpretable models, with the hope to measure the progress of the field. The tasks range from classification (sentiment, claim verification, conditional classification, entailment e-snli) to question answering (boolean QA, common sense questions - multiple choice); while the metrics focus on plausibility and faithfulness.&lt;/p&gt;

&lt;h2 id=&quot;see-also&quot;&gt;See also&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/document/d/1rQYAjY-jNKoQh8Z9-4NjqLF1_nAD7amr4sYH62eGPbU/edit#heading=h.lrgn79ao0for&quot;&gt;Selection of papers from ACL-2020&lt;/a&gt; by Yacine Jernite (HuggingFace)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@lawrence.carolin/interpretability-and-analysis-of-models-for-nlp-e6b977ac1dc6&quot;&gt;Interpretability and Analysis of Models for NLP @ ACL 2020&lt;/a&gt; by Carolin Lawrence&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/analytics-vidhya/highlights-of-acl-2020-4ef9f27a4f0c&quot;&gt;Highlights of ACL 2020&lt;/a&gt; by Vered Shwartz (AI2 &amp;amp; University of Washington)&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://europe.naverlabs.com/blog/ten-emerging-topics-at-acl-2020/&quot;&gt;Ten emerging topics at ACL 2020&lt;/a&gt; by NAVER LABS&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://towardsdatascience.com/knowledge-graphs-in-natural-language-processing-acl-2020-ebb1f0a6e0b1&quot;&gt;Knowledge Graphs in Natural Language Processing @ ACL 2020&lt;/a&gt; by Michael Galkin&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Taycir Yahmed</name>
        
        
      </author>

      

      

      
        <summary type="html">This post discusses highlights of the main conference of the 2020 Annual Meeting of the Association for Computational Linguistics (ACL 2020). The conference accepted 779 papers with an acceptance rate of 22.7%, had 25 tracks along with demo sessions, virtual meetups and mentoring sessions.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">ACL 2019 Highlights</title>
      <link href="/acl2019" rel="alternate" type="text/html" title="ACL 2019 Highlights" />
      <published>2019-08-01T21:03:36+00:00</published>
      <updated>2019-08-01T21:03:36+00:00</updated>
      <id>/acl2019</id>
      <content type="html" xml:base="/acl2019">&lt;p&gt;This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind.&lt;/p&gt;

&lt;p&gt;The conference accepted 660 &lt;a href=&quot;http://www.acl2019.org/EN/program.xhtml&quot;&gt;papers&lt;/a&gt; with an acceptance rate of 22.7%, had 6 parallel oral sessions plus one poster session and over 3K participants. It was sponsored by various industrial big players from the NLP research community (&lt;a href=&quot;http://www.acl2019.org/EN/exhibitors-sponsors.xhtml&quot;&gt;Full list of sponsors&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;The presidential address tackled several topics including the following list, that stuck out with me most:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Error analysis, interpretability and reproducibility&lt;/li&gt;
  &lt;li&gt;Multilinguality, low resource tasks and domain adaptation&lt;/li&gt;
  &lt;li&gt;Commonsense, reasoning and context aware modeling&lt;/li&gt;
  &lt;li&gt;Ethical NLP: Bias and environmental impact&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;error-analysis-interpretability-and-reproducibility&quot;&gt;Error analysis, interpretability and reproducibility&lt;/h2&gt;
&lt;p&gt;Deep Learning models are very sensitive to noisy input. In machine translation for instance, the variations of the user’s input can reveal bias in the language modeling and training data. To limit these drawbacks, &lt;a href=&quot;https://arxiv.org/pdf/1906.02443.pdf&quot;&gt;Cheng et al.&lt;/a&gt; (Google) suggests perturbing the model with adversarial inputs using an algorithm named &lt;strong&gt;Adversarial Generation&lt;/strong&gt; (AdvGen), which generates plausible adversarial examples and feeds them back into the model. This idea is inspired by GANs but doesn’t rely on a discriminator: instead it simply &lt;em&gt;augments&lt;/em&gt; the training data with these adversarial examples (more on AdvGen &lt;a href=&quot;https://ai.googleblog.com/2019/07/robust-neural-machine-translation.html&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;

&lt;p&gt;Interpretability also keeps coming back in different papers, showing a general interest in understanding and explaining the models’ inside mechanisms. &lt;a href=&quot;https://arxiv.org/abs/1906.03731&quot;&gt;Serrano et al.&lt;/a&gt; argues that attention wights don’t always identify information that models find important. Many experiments were performed to prove that attention weights mostly don’t correlate with the model outputs; whereas gradient-based rankings of attention weights better predict their effects (to further develop in future papers). On the other hand, &lt;a href=&quot;https://www.aclweb.org/anthology/P19-1284&quot;&gt;Bastings et al.&lt;/a&gt; show that sparse re-parameterized samples and unbiased gradients lead to effective latent rationales &lt;em&gt;(a short and informative part of the input text)&lt;/em&gt; for text classification. The approach consists in training two models, one latent model that selects the rationale to input in the second one, that performs the classification task.&lt;/p&gt;

&lt;p&gt;Equally, many papers try to explain models capabilities and low level representations. For instance, &lt;a href=&quot;https://arxiv.org/abs/1906.00592&quot;&gt;Yang et al.&lt;/a&gt; show that SANs (self-attention networks, popular for their parallelization and strong performance on various NLP tasks, such as machine translation) can’t efficiently learn the positional information even with the position embedding, when trained on a word reordering detection task. Nevertheless, SANs learn better positional information than RNNs when trained on a different downstream task such as MT. When it comes to &lt;em&gt;Multi-head self-attention&lt;/em&gt;, &lt;a href=&quot;https://arxiv.org/abs/1905.09418&quot;&gt;Voita et al.&lt;/a&gt; show that the most important (in magnitude) and confident heads play consistent and often linguistically interpretable roles. Furthermore, they prove that pruning heads (using a method based on stochastic gates and a differentiable relaxation of the L0 penalty) leads to remove the vast majority without (or with minimum) impact on the performance. More generally, &lt;a href=&quot;https://hal.inria.fr/hal-02131630/document&quot;&gt;Jawaharet et al.&lt;/a&gt; (Inria) investigate which structure of language BERT learns. They show that, in the lower layers, BERT learns phrase-level information and in the intermediate layers a hierarchy of linguistic information (surface features at the bottom, syntactic features in the middle and semantic features at the top). They also prove that BERT requires deeper layers to encode long-distance information (e.g. track subject-verb agreement).&lt;/p&gt;

&lt;p&gt;Further details and resources about interpretability will be shared in the &lt;a href=&quot;https://blackboxnlp.github.io/&quot;&gt;BlackboxNLP&lt;/a&gt; workshop, following the main conference. Live streaming &lt;a href=&quot;https://www.youtube.com/embed/JEH2fiyjrJU&quot;&gt;link&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;On another note, there is a growing interest in &lt;strong&gt;robust evaluation&lt;/strong&gt; and reproducibility among the NLP community. Indeed, the benchmark leaderboards race often comes with small core changes and “public hyper-parameters search with more training data” (e.g. RoBERTa), which leads to over-fitting benchmark datasets and drives the attention away from more fundamental research and new idea. One of the selected “outstanding papers” deals with this issue: &lt;a href=&quot;http://wellformedness.com/papers/gorman-bedrick-2019.pdf&quot;&gt;Gorman et al.&lt;/a&gt; show that system rankings based on standard splits fail to reproduce, and recommend “Bonferroni-corrected random split hypothesis testing” instead. The best demo paper also provides an open-source framework for machine translation quality estimation called &lt;a href=&quot;https://unbabel.github.io/OpenKiwi&quot;&gt;OpenKiwi&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;multilinguality-low-resource-tasks-and-domain-adaptation&quot;&gt;Multilinguality, low resource tasks and domain adaptation&lt;/h2&gt;
&lt;p&gt;Since last year, an interest in multilingual training to support low-resource languages has be pronounced. Indeed, most of the NLP benchmarks and SOTA results are reported on English, whereas in real life applications other languages are used and needed. That led a wave of cross-lingual and multilingual research results. &lt;a href=&quot;https://arxiv.org/abs/1906.05407&quot;&gt;Ormazabal et al.&lt;/a&gt; argue that most cross-lingual embeddings are learnt with offline methods (learn embeddings in different languages then map them to a shared space) which underlay the &lt;em&gt;isomorphism assumption&lt;/em&gt;, stating that embeddings in different languages have the same structure. They investigate whether this issue is due to the mapping or learning of embeddings. Eventually, they prove that joint learning (using an extension of skip-gram model with bilingual data) yields more isomorphic embeddings and better bilingual lexicon induction. Thus, they conclude that mapping methods have strong limitation and call for more research in joint learning. Indeed, so many research group have been interested in aligning cross-lingual embeddings to build bilingual dictionaries and induce word translation pairs through nearest neighbor or related retrieval methods. Furthermore, &lt;a href=&quot;https://arxiv.org/abs/1907.10761&quot;&gt;Artetxe et al.&lt;/a&gt; suggest inducing bilingual embeddings through a different approach: It consist of generating a synthetic parallel corpus  through a phrase based translation system (built using cross-lingual embeddings) and then extract the bilingual lexicon using statistical word alignment methods. Thus, they use no more resources than the monolingual corpus used to learn the embeddings.&lt;/p&gt;

&lt;p&gt;The works on cross-lingual and multilingual NLP led the community to adopt a more stimulating position about presenting papers and results for various languages. The Bender Rule (ref. Emily M. Bender) states that any work should state the language they use and not fall to English by default, since that emphasizes English as a reference language (though it doesn’t capture multilingual variations) and puts other languages in a second stand. On the one hand, &lt;a href=&quot;https://arxiv.org/abs/1906.04726&quot;&gt;Mielke, et al.&lt;/a&gt; investigate languages that are harder to model, given the observation that SOTA methods do not perform equally well on high resource languages (from the Europarl corpus). To do that, they start by defining an objective measure of difficulty using parallel corpora covering 69 languages (13 language families) and propose a paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients, handling missing data and inter-sentence variations. On the other hand, &lt;a href=&quot;https://arxiv.org/abs/1902.00193&quot;&gt;Rahimi et al.&lt;/a&gt; propose a “massively” multilingual transfer for NER models, applied to emergency response. They show that, when using cross-lingual embeddings, some language pairs ensure better performance than the ensemble voting models, yielding unexpected language pairs (Indonesian is best transfered from Italian - inducing that these results depend on the used embeddings). Although, the transfer results are best within language families, they remain overall noisy, thus they suggest applying transfer from multiple source languages. The main idea is to encode the input sentence to a cross-lingual representation, then use models trained on high resource languages with cross-lingual representations, to predict the tags. More details in the &lt;a href=&quot;https://docs.google.com/presentation/d/13y31FNC-LTwENdOiWUajVeusuE-sFgcmz3hO8qHdh90/&quot;&gt;slides&lt;/a&gt;. In another work about multilingual BERT, &lt;a href=&quot;https://arxiv.org/abs/1906.01502&quot;&gt;Pires et al.&lt;/a&gt; show that transfer doesn’t really depend on vocabulary overlap but rather on typological similarity (SVO vs. SOV), indeed they noticed a performance drop when changing word order. Further more, they prove that mBERT is good at transfering mixed-language (code switching) but not transliterated targets. Finally, they confirm that translations have similar representations.&lt;/p&gt;

&lt;p&gt;Furthermore, in the presidential address, Ming Zhou listed the topics to work on low-resource tasks including: &lt;strong&gt;Transfer learning, Unsupervised learning, Cross-language learning and Prior knowledge with human in the loop.&lt;/strong&gt; For instance, &lt;a href=&quot;https://arxiv.org/abs/1905.08212&quot;&gt;Wang et al.&lt;/a&gt; suggest using the most related high resource language to improve low resource NMT, instead of using all available multilingual data. Moreover, they prove that using an &lt;em&gt;intelligent&lt;/em&gt; selection method from other auxiliary languages can further enhance the performance. To do so, they propose an algorithm dabbed Target Conditioned Sampling (TCS), which first samples a target sentence, and then conditionally samples its source sentence. On a different note, &lt;a href=&quot;https://www.cis.uni-muenchen.de/~fraser/pubs/huck_acl2019.pdf&quot;&gt;Huck et al.&lt;/a&gt; propose an approach to better translate OOVs (Out Of Vocabulary words) in MT systems. Indeed, these words are usually represented with BPE tokens, but that can lead to bad translations, for languages like German. The main idea, here, is to translate the OOVs using bilingual embeddings and keeping track of 5 candidate translations, then back-translate the targets while forcing the target OOV to translate to the source OOV. By fine-tuning the model with this synthetic data, they report better OOVs translation performance. Moreover, &lt;a href=&quot;https://arxiv.org/pdf/1906.03785.pdf&quot;&gt;Xia et al.&lt;/a&gt; suggest an approach to use monolingual data when training low-resource translation models, that pivots through a &lt;strong&gt;related&lt;/strong&gt; high-resource language (HRL). First, they inject LRL words in HRL sentences using bilingual dictionaries. Second, they edit the modified sentences using an unsupervised MT framework.  They show that the proposed method outperforms back-translation. Moreover, in an attempt to reuse elaborated methods in low-resource tasks,  &lt;a href=&quot;https://arxiv.org/abs/1905.09135&quot;&gt;Beryozkin et al.&lt;/a&gt; (Google) use a tag hierarchy to adapt a pre-trained NER model with no additional training data. The main idea consists in constructing a tag hierarchy, training the model with the highest level of tags as targets and back-propagating with the fine-grained tags.&lt;/p&gt;

&lt;p&gt;As of speech translation, recent papers showed that it was possible to achieve with end-to-end models (avoiding cascaded models: STT ➜ MT) but considering the &lt;em&gt;unrealistic&lt;/em&gt; assumption of equal size of training data. &lt;a href=&quot;https://arxiv.org/abs/1904.07209&quot;&gt;Sperber et al.&lt;/a&gt; show that end-to-end speech translation models require more data to achieve the same performance as cascaded models. To solve this problem, they propose training end-to-end multi-task models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. Furthermore, they introduce an attention-passing technique that alleviates error propagation issues.&lt;/p&gt;

&lt;h2 id=&quot;commonsense-reasoning-and-context-aware-modeling&quot;&gt;Commonsense, reasoning and context aware modeling&lt;/h2&gt;
&lt;p&gt;This topic comes back in each session dealing with QA; it seems to be more challenging and with a certain latency between the research results and the industry applications. Indeed, the research community is focusing more on seq2seq models - that do not ensure a perfect user experience, both in chit-chat and task oriented systems. Consequently industrial systems rely more on NLU, DST, DM and NLG components. In the second keynote, Pascal Fung explained various aspects related to QA systems and the ethical challenges they raise, including presenting them as humans deceiving the user at the other end, in certain use cases.&lt;/p&gt;

&lt;p&gt;Research groups in companies as well as in academia are focusing efforts on building meaningful datasets, intended to make QA / reasoning systems closer to the end-user, for example Google Research has presented a dataset paper with user formulated requests matched to their answers from wikipedia articles, with two fine grained levels: paragraph containing the answer and entity corresponding to the exact answer to the question. This is pushing end-to-end systems to deal directly with the user’s query and infer the exact information to look for. Here is the &lt;a href=&quot;https://ai.google.com/research/NaturalQuestions/&quot;&gt;link&lt;/a&gt; to the data. On the one hand, &lt;a href=&quot;https://arxiv.org/pdf/1905.07098.pdf&quot;&gt;Xiong et al.&lt;/a&gt; propose a new end-to-end question answering model, which learns to aggregate answer evidence from an incomplete knowledge base (KB) and a set of retrieved text snippets. On the other hand, &lt;a href=&quot;https://arxiv.org/pdf/1906.04980.pdf&quot;&gt;Lewis et al.&lt;/a&gt; follow the insights from Lample et el. on unsupervised machine translation, to investigate unsupervised extractive QA. They suggest various methods to perform cloze to natural question translation and show that modern QA systems have a good performance even when trained using only synthetic data. Moreover, &lt;a href=&quot;https://arxiv.org/abs/1905.13453&quot;&gt;Talmor et al.&lt;/a&gt;  investigate generalization and transfer in reading comprehension tasks and show that training on a source RC dataset and transferring to a target dataset substantially improves performance. They propose MultiQA, a BERT-based model, trained on multiple RC datasets, which leads to state-of-the-art performance on 5 RC datasets.&lt;/p&gt;

&lt;p&gt;Moreover, there seems to be an important interest in building systems with in-domain data that ensure context aware predictions. Studies show that systems lacking context in real life applications (away from the SOTA and leaderboard benchmarks on very specific and studied datasets) can lead to biased decision making and erroneous predictions. &lt;a href=&quot;https://homes.cs.washington.edu/~skgabrie/sap2019risk.pdf&quot;&gt;Sap et al.&lt;/a&gt; show that systems trained on out of domain data lead to strongly biased decisions and dangerous censorship.&lt;/p&gt;

&lt;p&gt;On a different note, the transformer-xl paper by &lt;a href=&quot;https://arxiv.org/abs/1901.02860&quot;&gt;Dai et al.&lt;/a&gt; was presented during the conference. The main idea consists in surpassing the fixed-length context, (a long text sequence is truncated into fixed-length segments processed separately), using two techniques: a &lt;em&gt;segment-level recurrence mechanism&lt;/em&gt; and a &lt;em&gt;relative positional encoding scheme&lt;/em&gt;. More details &lt;a href=&quot;https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html&quot;&gt;here&lt;/a&gt;. Furthermore, the first keynote, by Liang Huang, introduces a new architecture dabbed prefix-to-prefix as an evolution of sequence-to-sequence to take into consideration the temporal evolution of the captured context in simultaneous translation. More in the &lt;a href=&quot;https://arxiv.org/abs/1906.01135&quot;&gt;paper&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;ethical-nlp-bias-and-environmental-impact&quot;&gt;Ethical NLP: Bias and environmental impact&lt;/h2&gt;
&lt;p&gt;There was a significant number of papers dealing with debiasing NLP methods at ACL 2019. Indeed, recent studies have shown that pre-trained word embeddings and NLP resources hold a certain level of gender and geographical bias, among others. Since these models can be used in decision making models, ethical concerns have emerged, illustrated by the wide interest of the research community. For instance, &lt;a href=&quot;https://arxiv.org/abs/1906.08976&quot;&gt;Sun et al.&lt;/a&gt; present a literature overview of gender bias mitigating techniques. Furthermore, &lt;a href=&quot;https://arxiv.org/abs/1906.04571&quot;&gt;Zmigrod et al.&lt;/a&gt; show that commonly employed debiasing approaches produce ungrammatical sentences in morphologically rich languages and present a novel approach for converting between masculine-inflected and feminine-inflected sentences. They test their approach on 4 languages showing bias reduction without harming grammaticality. On demographic bias, &lt;a href=&quot;https://www.aclweb.org/anthology/P19-1162&quot;&gt;Sweeney et al.&lt;/a&gt; argue that most demographic bias evaluation approaches rely on vector space based metrics like the &lt;em&gt;Word Embedding Association Test (WEAT)&lt;/em&gt;, which doesn’t measure the impact on downstream tasks. They suggest a new metric (Relative Negative Sentiment Bias, RNSB) to measure the relative negative sentiment associated with demographic identity terms. Moreover, a position paper by &lt;a href=&quot;https://arxiv.org/abs/1906.01738&quot;&gt;Jurgens et al.&lt;/a&gt; argues that the community needs to make three substantive changes to address online abuse: first, expanding the scope of problems to tackle both more subtle and more serious forms of abuse, second, developing proactive technologies that counter or inhibit abuse before it harms, and third reframing the efforts within a framework of justice to promote healthy communities.&lt;/p&gt;

&lt;p&gt;More details on the workshop for &lt;a href=&quot;https://genderbiasnlp.talp.cat/&quot;&gt;Gender Bias in Natural Language Processing&lt;/a&gt; and &lt;a href=&quot;http://www.winlp.org/winlp-2019-workshop/&quot;&gt;WiNLP&lt;/a&gt; workshop.&lt;/p&gt;

&lt;p&gt;Another interesting paper by &lt;a href=&quot;https://arxiv.org/abs/1906.02243&quot;&gt;Strubell et al.&lt;/a&gt; illustrates the environmental impact of training and especially tuning SOTA NLP models. The authors quantify the approximate financial and environmental costs of training a variety of recent neural network models for NLP and propose actionable recommendations to reduce costs and improve equity in NLP research and practice.&lt;/p&gt;

&lt;h2 id=&quot;see-also&quot;&gt;See also&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.livecongress.it/sved/evt/aol_lnk.php?id=60B5FD70&quot;&gt;Video recordings&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://www.cs.cmu.edu/~neulab//2019/07/25/neulab-presentations-at-acl-2019.html&quot;&gt;NeuLab Presentations at ACL 2019&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.google.com/presentation/d/1miPu0hkBFSRTAxP-Psg0xXOvAbVSJPhQXehMQdUI88I/edit#slide=id.p&quot;&gt;Unsupervised Cross-lingual Representation Learning tutorial&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Best papers nominations &lt;a href=&quot;http://www.acl2019.org/EN/nominations-for-acl-2019-best-paper-awards.xhtml&quot;&gt;here&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.mihaileric.com/posts/nlp-trends-acl-2019/&quot;&gt;Trends in Natural Language Processing: ACL 2019 In Review&lt;/a&gt; by Mihail Eric.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@mgalkin/knowledge-graphs-in-natural-language-processing-acl-2019-7a14eb20fce8&quot;&gt;Knowledge Graphs in Natural Language Processing @ ACL 2019&lt;/a&gt; by Michael Galkin.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://noecasas.com/post/acl2019/&quot;&gt;Notes on ACL 2019&lt;/a&gt; by Noe Casas.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@soramas/acl-2019-my-take-home-messages-a94bc00e9896&quot;&gt;ACL 2019, my take home messages&lt;/a&gt; by Sergio Oramas.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/@mariekhvalchik/acl-2019-1adf4c748711&quot;&gt;ACL 2019: Highlights and Trends&lt;/a&gt; by Maria Khvalchik.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://medium.com/syncedreview/acl-2019-best-papers-announced-e0141024a935&quot;&gt;ACL 2019 Best Papers Announced&lt;/a&gt; by Synced.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://supernlp.github.io/2019/08/16/acl-2019/&quot;&gt;ACL 2019 Thoughts and Notes&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Taycir Yahmed</name>
        
        
      </author>

      

      

      
        <summary type="html">This post discusses highlights of the main conference of the 2019 Annual Meeting of the Association for Computational Linguistics (ACL 2019). Note that these notes are written with business applications in mind.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">mcQA - Multiple Choice Question Answering</title>
      <link href="/mcQA" rel="alternate" type="text/html" title="mcQA - Multiple Choice Question Answering" />
      <published>2019-07-10T07:25:36+00:00</published>
      <updated>2019-07-10T07:25:36+00:00</updated>
      <id>/mcQA</id>
      <content type="html" xml:base="/mcQA">&lt;p&gt;mcQA is a multiple choice question answering python library, using Language Models.&lt;/p&gt;

&lt;h2 id=&quot;installation&quot;&gt;Installation&lt;/h2&gt;

&lt;h3 id=&quot;with-pip&quot;&gt;With pip&lt;/h3&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;pip install mcqa
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;from-source&quot;&gt;From source&lt;/h3&gt;

&lt;div class=&quot;language-shell highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/mcqa-suite/mcqa.git
&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;mcQA
pip install &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;getting-started&quot;&gt;Getting started&lt;/h2&gt;

&lt;h3 id=&quot;data-preparation&quot;&gt;Data preparation&lt;/h3&gt;

&lt;p&gt;To train a &lt;code class=&quot;highlighter-rouge&quot;&gt;mcQA&lt;/code&gt; model, you need to create a csv file with n+2 columns, n being the number of choices for each question. The first column should be the context sentence, the n following columns should be the choices for that question and the last column is the selected answer.&lt;/p&gt;

&lt;p&gt;Below is an example of a 3 choice question (taken from the &lt;a href=&quot;https://arxiv.org/pdf/1906.02361.pdf&quot;&gt;CoS-E dataset&lt;/a&gt;) :&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Context sentence&lt;/th&gt;
      &lt;th&gt;Choice 1&lt;/th&gt;
      &lt;th&gt;Choice 2&lt;/th&gt;
      &lt;th&gt;Choice 3&lt;/th&gt;
      &lt;th&gt;Label&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;People do what during their time off from work?&lt;/td&gt;
      &lt;td&gt;take trips&lt;/td&gt;
      &lt;td&gt;brow shorter&lt;/td&gt;
      &lt;td&gt;become hysterical&lt;/td&gt;
      &lt;td&gt;take trips&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;If you have a trained &lt;code class=&quot;highlighter-rouge&quot;&gt;mcQA&lt;/code&gt; model and want to infer on a dataset, it should have the same format as the train data, but the &lt;code class=&quot;highlighter-rouge&quot;&gt;label&lt;/code&gt; column.&lt;/p&gt;

&lt;p&gt;See example data preparation below:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mcqa.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCQAData&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mcqa_data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MCQAData&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bert_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert-base-uncased&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;lower_case&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;max_seq_length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
                     
&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mcqa_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'swagaf/data/train.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;test_dataset&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mcqa_data&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;data_file&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'swagaf/data/test.csv'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;is_training&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;model-training&quot;&gt;Model training&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mcqa.models&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;

&lt;span class=&quot;n&quot;&gt;mdl&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;bert_model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;bert-base-uncased&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;cuda&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; 
            
&lt;span class=&quot;n&quot;&gt;mdl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fit&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;train_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;num_train_epochs&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;prediction&quot;&gt;Prediction&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;mdl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;test_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;eval_batch_size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;evaluation&quot;&gt;Evaluation&lt;/h3&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;sklearn.metrics&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;
&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;mcqa.data&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_labels&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;print&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;accuracy_score&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;preds&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;get_labels&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;train_dataset&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1906.02361.pdf&quot;&gt;Explain Yourself! Leveraging Language Models for Commonsense Reasoning&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1808.05326&quot;&gt;SWAG: A Large-Scale Adversarial Dataset for Grounded Commonsense Inference&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1908.07898.pdf&quot;&gt;Are We Modeling the Task or the Annotator? An Investigation of Annotator Bias in Natural Language Understanding Datasets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1809.02789&quot;&gt;Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1811.00937&quot;&gt;CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/abs/1704.04683&quot;&gt;RACE: Large-scale ReAding Comprehension Dataset From Examinations&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtube.com/watch?v=yIdF-17HwSk&quot;&gt;Stanford CS224N: NLP with Deep Learning Lecture 10 – Question Answering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Taycir Yahmed</name>
        
        
      </author>

      

      

      
        <summary type="html">mcQA is a multiple choice question answering python library, using Language Models.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Building Parallel Corpora Using Cross-Lingual BOW</title>
      <link href="/Building-Parallel-Corpora-CLBOW" rel="alternate" type="text/html" title="Building Parallel Corpora Using Cross-Lingual BOW" />
      <published>2018-07-13T07:25:36+00:00</published>
      <updated>2018-07-13T07:25:36+00:00</updated>
      <id>/Building-Parallel-Corpora-CLBOW</id>
      <content type="html" xml:base="/Building-Parallel-Corpora-CLBOW">&lt;p&gt;Training machine translation models requires a huge amount of parallel data.
Consequently, there has been many works suggesting different methods to build
bilingual corpora, leading to the construction of reliable training datasets for
machine translation systems.&lt;/p&gt;

&lt;p&gt;However, the problem is still prominent for the below use-cases:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Low-resource setup: Although for some language pairs, we have parallel datasets with a convenient size (e.g. around 50M sentences for French - English), this is not the case for all language pairs. Indeed, low resource languages do not have as much parallel data making it hard to
train reliable translation models to and from these languages.&lt;/li&gt;
  &lt;li&gt;Specialization setup: Furthermore, machine translation is sensitive to context. Thus, any available specialized data can have a strong influence on the model’s performance for a specific domain. For instance, using medical data when training the model enhances its performance on prescriptions’ translation. Note that there are various &lt;a href=&quot;https://arxiv.org/abs/1612.06140&quot;&gt;domain control&lt;/a&gt; strategies for machine translation, such as adding the domain tag as an additional feature or adding a special token to the sentence when training and translating; this is not, however, the core of this article.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Due to the aforementioned reasons, there is still room for designing and implementing solutions for building parallel corpora. In the following sections, I present a solution for matching multilingual documents in order to construct a parallel corpus.&lt;/p&gt;

&lt;h2 id=&quot;clbow-cross-lingual-bag-of-words&quot;&gt;CLBOW: Cross-Lingual Bag-Of-Words&lt;/h2&gt;

&lt;p&gt;When designing an algorithm to match cross-lingual documents, the first reflex is
to represent all available documents in numerical vectors. However, to compare
these documents, the vectorial representations should be language-independent
or cross-lingual, meaning that semantically similar documents should be close
in the multidimensional representation space.&lt;/p&gt;

&lt;p&gt;Although most recent research &lt;a href=&quot;https://arxiv.org/abs/1710.04087&quot;&gt;works&lt;/a&gt; focus on multilingual word embeddings as a numerical representation of text data, here we present a generalization of
Bag-Of-Words to a cross-lingual setup, where we represent all documents in the same space irrespectively of their language. Below is the explicit implementation of the algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/algo1.png&quot; alt=&quot;Implementation of CLBOW: Cross-Lingual Bag of Words&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Illustration of Cross-Lingual Bag-Of-Words (CLBOW):&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/cbow.png&quot; alt=&quot;Illustration of CLBOW: Cross-Lingual Bag-Of-Words&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Notes:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;In the above illustration, the decoding of only two languages is presented for simplicity purposes; nevertheless the suggested implementation is extended to many languages.&lt;/li&gt;
  &lt;li&gt;Furthermore, it can handle polysemy since at each decoding step t, not only one translation of the word wt is considered but its different translations.&lt;/li&gt;
  &lt;li&gt;This version of BOW can provide both binary and numerical representation of the documents. By numerical, I refer to the extension of TF-IDF (Term Frequency - Inverse Document Frequency) to a cross-lingual setup.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;application-to-parallel-corpora-construction&quot;&gt;Application to parallel corpora construction&lt;/h2&gt;
&lt;p&gt;Thanks to the previous algorithm, cross-lingual vectorial representations of the documents are calculated. Afterwards, a search for the closest document of a different language is performed using the minimization of the cosine distance and with regards to a threshold corresponding to the typical length ratio for the language pair. For instance, this threshold is equal to 1.5 for French-English bilingual corpora. A maximum accepted distance between a document and a candidate translated version is also considered, to discriminate documents having the same template (headers, footers, etc.). In my various experiments, this threshold is equal to 0.6.&lt;/p&gt;

&lt;p&gt;Using this method, classes of equivalence representing each the multilingual
versions of the same document are retrieved. For example, a class of equivalence
can be represented as the following:&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Regle FR 29-01-2018.pdf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
 &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'CS1548325.pdf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; 
 &lt;span class=&quot;s&quot;&gt;'pt'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Regra 29-01-2018.pdf'&lt;/span&gt; 
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fr'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'Doc 12052005.pdf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'en'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'To print.pdf'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;
 &lt;span class=&quot;s&quot;&gt;'de'&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'pr12052005.pdf'&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Below is the detailed algorithm:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/algo2.png&quot; alt=&quot;Multilingual document matching&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To build bilingual corpora, I consider sequentially pairs of languages. Then on each pair of documents, I apply sentence alignment using the algorithm &lt;a href=&quot;http://mt-archive.info/AMTA-2010-Sennrich.pdf&quot;&gt;BLEUAlign&lt;/a&gt;. This will provide a bilingual parallel corpus for each data source relevant to a specific domain. These corpora are then used to train and specialize machine translation systems and using them enabled a good enhancement in &lt;a href=&quot;https://www.aclweb.org/anthology/P02-1040&quot;&gt;BLEU&lt;/a&gt; score. Generally, if ∆BLEU is the difference between the BLEU on a standard dataset and a specialized dataset of the general model, you should expect to gain around ∆BLEU on the specialized dataset using the augmented model.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;The here-presented pipeline enabled the construction of a specialized bilingual corpus, that I used to enhance the performance of translation models both on standard datasets and on specialized data (financial, medical, etc.). Other improvements are however to be tested in the near future, including neural encoding of multilingual documents.&lt;/p&gt;</content>

      
      
      
      
      

      <author>
          <name>Taycir Yahmed</name>
        
        
      </author>

      

      

      
        <summary type="html">Training machine translation models requires a huge amount of parallel data. Consequently, there has been many works suggesting different methods to build bilingual corpora, leading to the construction of reliable training datasets for machine translation systems.</summary>
      

      
      
    </entry>
  
    <entry>
      <title type="html">Clause Augmentation for Better NMT</title>
      <link href="/Clause-Augmentation-for-Better-NMT" rel="alternate" type="text/html" title="Clause Augmentation for Better NMT" />
      <published>2018-04-01T07:25:36+00:00</published>
      <updated>2018-04-01T07:25:36+00:00</updated>
      <id>/Clause-Augmentation-for-Better-NMT</id>
      <content type="html" xml:base="/Clause-Augmentation-for-Better-NMT">&lt;p&gt;Most public parallel corpora are formed of long sentences. Consequently, neural translation models tend to generate a long output with n-grams repetition, even when they are exposed to a short sequence or a one-word example. This causes the repetition problem, explained by the fact that none of the neurons learns the representation of length, thus the model generates a long sequence by default. In other terms, the probability of appearance of the end-of-sentence token &lt;code class=&quot;highlighter-rouge&quot;&gt;&amp;lt;eos&amp;gt;&lt;/code&gt; will not be high enough to stop the output generation when translating a short sequences.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Illustration of n-grams repetition on clauses translation:&lt;/em&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Source:&lt;/strong&gt; et il croit depuis lors a un taux de 5 %&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Translation:&lt;/strong&gt; since then and since then at 5 % at 5 %&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To solve this problem, a possible solution is augmenting the training parallel corpus with sequences of a smaller length, typically one-word examples (using bilingual dictionaries) and sub-sentences. To generate the sub-sentences, two important steps are considered:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;First, detect and segment clauses in long sentences.&lt;/li&gt;
  &lt;li&gt;Second, retrieve the clauses exact translation.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;clause-detection-and-segmentation&quot;&gt;Clause detection and segmentation&lt;/h2&gt;
&lt;p&gt;In neural machine translation, sentences with more that 50 tokens are usually dropped. According to many research &lt;a href=&quot;https://arxiv.org/abs/1409.0473&quot;&gt;papers&lt;/a&gt;, sentences with such length harm the performance. As a consequence, an approach is suggested to segment these sentences to clauses and thus use them while training instead of simply dropping them. The first task is detecting clauses in long sentences. To do so, linguistic rules, specific to each language that mark the beginning / end of a clause, are needed.
These rules are formulated within a Treebank.&lt;/p&gt;

&lt;p&gt;In linguistics, a treebank is a syntactic or semantic sentence structure annotator. The introduction of the first parsed corpora in the 90s, revolutionized computational linguistics, particularly after publishing &lt;a href=&quot;https://repository.upenn.edu/cgi/viewcontent.cgi?article=2068&amp;amp;context=cis_reports&quot;&gt;Penn Treebank&lt;/a&gt;, the first large-scale treebank. Indeed, annotated treebank data has been crucial in syntactic research to test linguistic theories of sentence structure. In addition, there are variants of treebanks, including phrase structure annotators and dependency structure annotators. Note that in these experiments, phrase structure annotators are used.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/anno.jpg&quot; alt=&quot;Variants of syntactic treebanks&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Since this experiment deals with French to English translation scenario, a French Treebank is needed. Due to license constraints and the need for phrase annotators, Paris 7 French Treebank was chosen. This Treebank was initiated in 1997, with the collaboration of IUF, CNRS and CNRTL. It consists of 1 million words of the newspaper Le Monde (1989-1995). The full list of the generated tags is accessible &lt;a href=&quot;http://www.llf.cnrs.fr/Gens/Abeille/French-Treebank-fr.php&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Clauses segmentation:&lt;/strong&gt; The first step is identifying the usually dropped sentences, those with more than 50 tokens (words). Afterwards, these sentences are annotated each with phrase tags using the French treebank. Below is an example for both French and English:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/tree.png&quot; alt=&quot;Clause detection and segmentation: French and English examples&quot; width=&quot;800&quot; /&gt;&lt;/p&gt;

&lt;p&gt;To select the clauses, specific tags are selected:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Selected tags for English:
    &lt;ul&gt;
      &lt;li&gt;S: simple declarative clause, i.e. one that is not introduced by a subordinating conjunction or a wh-word and that does not exhibit subject-verb inversion.&lt;/li&gt;
      &lt;li&gt;SBAR: Clause introduced by a subordinating conjunction.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Selected tags for French:
    &lt;ul&gt;
      &lt;li&gt;Ssub: subordinate clause (“completive”, indirect interrogative, circumstantial subordinate)&lt;/li&gt;
      &lt;li&gt;Sint: clause “conjuguee interne” (coordinated, direct speech, incise)&lt;/li&gt;
      &lt;li&gt;PP: prepositional phrase&lt;/li&gt;
      &lt;li&gt;Srel: relative proposition (starting with a relative pronoun)&lt;/li&gt;
      &lt;li&gt;COORD: coordinated phrase&lt;/li&gt;
      &lt;li&gt;VPinf: infinitive proposition (starting with a preposition)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Using these tags, the long sentences are segmented to the clauses that form them. So, whenever a new tag is encountered, when visiting the different nodes of the parsing tree, a new clause is generated. This segmentation step results in a corpus of short sequences in the source language. Now, the exact translations for these clauses have to be generated.&lt;/p&gt;

&lt;h2 id=&quot;synthetic-translation-of-the-extracted-clauses&quot;&gt;Synthetic translation of the extracted clauses&lt;/h2&gt;
&lt;p&gt;To translate the clauses, the original model can’t be used because it doesn’t handle short sequence translations and would generate n-gram repetition. However, in this section, a method allowing quality translation for sub-sentences is presented. Eventually, this proposed approach generates a bilingual corpus of short sequences / phrases.&lt;/p&gt;

&lt;p&gt;Below are the different steps applied to get the clauses’ translation:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Using the original model, translate the original long sentences, from which we previously extracted the clauses.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Extract the attention weights generated by the previous translations. The attention weights are denoted αij , representing the contribution of word i on the source side in the translation of word j in the target side. Note that i ranges from 1 to length of the source sentence, here denoted n; j ranges from 1 to length of the target sentence, here denoted m. See below an example of attention weights generated with translation:*&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/Image1.png&quot; alt=&quot;Attention weights generated with translation&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Here the source sentence is “Nous esperons qu’ il s’agit la d’une preuve de sa pertinence politique.” and the target prediction is “We hope that this is proof of its political relevance.”. Each cell αij , where 1 ≤ i ≤ n, 1 ≤ j ≤ m: n being the length of the source sentence and m being the length of the target sentence, represents the contribution of target word j in the translation of the source word i. Note that the lighter the cell, the more important the attention weight. &lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

    &lt;p&gt;&lt;strong&gt;Important remark:&lt;/strong&gt; Here, the matrix is predominantly &lt;strong&gt;diagonal&lt;/strong&gt;: this indicates how much the French and English languages are aligned. An example of the attention matrix corresponding to non-aligned languages (Japanese to English) can be seen in the below figure.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/Image12.png&quot; alt=&quot;Attention matrix of Japanese to English translation&quot; width=&quot;400&quot; /&gt;&lt;/p&gt;

    &lt;p&gt;Furthermore, some &lt;strong&gt;anti-diagonal&lt;/strong&gt; sections in the matrix can be observed, these are due to the difference in the order of adjective compounds between French and English, e.g. “pertinence politique” is translated to “political relevance”.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Apply the following algorithm to retrieve the clauses’ translation.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/algo.png&quot; alt=&quot;Clauses synthetic translation&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; the thresholds 0.4 and 0.7 are selected after experiments on the alignment
between French and English languages. See below an illustration of synthetic translation of the first clause “Nous esperons”:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/Image2.png&quot; alt=&quot;Illustration of synthetic translation of the first clause 'Nous esperons'&quot; width=&quot;700&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note that, in the graph on the right, the horizontal axis represents the position of words in the target sentence and the vertical axis represents the contribution of the corresponding target word in the translation of the clause (here: “Nous esperons”). The image below illustrates the &lt;em&gt;Information transfer through the source and target sentences:&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/transfer.png&quot; alt=&quot;Information transfer through the source and target sentences.&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;model-training-with-clauses&quot;&gt;Model training with clauses&lt;/h2&gt;
&lt;p&gt;Using the previously described processes, a bilingual corpus of clauses is constructed. However, in the following experiment, only 35,821 clauses are used, which makes around 3% of the available clauses. Furthermore, each set of clauses is concatenated to the corresponding corpus among the source (French) and the target (English). Afterwards, the two corpora are jointly randomized so that
the clauses are not located just in the end of the data set, but spread along the corpus. Then, the model is retrained during 13 epochs with the baseline setup: 2.5 million parallel sentences, 4 bidirectional LSTM attentional encoder-decoder architecture with 500 as embedding size, 500 as number of hidden units and 5 as beam size.&lt;/p&gt;

&lt;h2 id=&quot;results-and-discussion&quot;&gt;Results and discussion&lt;/h2&gt;
&lt;p&gt;Below, I present the scores obtained using this method on WMT 2015 test set and on a test set of clauses out-of-sample.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Experiment       &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;WMT BLEU      &lt;/th&gt;
      &lt;th style=&quot;text-align: center&quot;&gt;Clauses BLEU&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;Baseline       &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;28.41      &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;49.73&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;Augmented model      &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;28.85      &lt;/td&gt;
      &lt;td style=&quot;text-align: center&quot;&gt;58.31&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;&lt;strong&gt;Quantitative discussion:&lt;/strong&gt; Integrating the clauses improves the performance with 0.44 BLEU on WMT 2015 and 8.58 BLEU on a test set of clauses. Note that in this experiment, only 3% of the available clauses are used. &lt;br /&gt;&lt;br /&gt;
&lt;strong&gt;Qualitative discussion:&lt;/strong&gt; Integrating the clauses has an influence mostly on translating short sequences. The method was suggested to solve the problem of n-gram repetition and indeed it did. Below is an example illustrating how the augmented model translates short sequences:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Source:&lt;/strong&gt; et il croit depuis lors a un taux de 5 % &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Baseline translation:&lt;/strong&gt; since then and since then at 5 % at 5 % &lt;br /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Augmented model:&lt;/strong&gt; and it has been growing since then at a rate of 5 % &lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;</content>

      
      
      
      
      

      <author>
          <name>Taycir Yahmed</name>
        
        
      </author>

      

      

      
        <summary type="html">Most public parallel corpora are formed of long sentences. Consequently, neural translation models tend to generate a long output with n-grams repetition, even when they are exposed to a short sequence or a one-word example. This causes the repetition problem, explained by the fact that none of the neurons learns the representation of length, thus the model generates a long sequence by default. In other terms, the probability of appearance of the end-of-sentence token &amp;lt;eos&amp;gt; will not be high enough to stop the output generation when translating a short sequences.</summary>
      

      
      
    </entry>
  
</feed>
